{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqINAqfx133qemSg+TKG5Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/byrcewang/DL_SS2H/blob/main/Transformer_PyTorch_big.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Step 1: Define the Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len=512):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        position = torch.arange(0, max_seq_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        self.positional_encoding = torch.zeros(1, max_seq_len, d_model)\n",
        "        self.positional_encoding[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        self.positional_encoding[0, :, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.positional_encoding\n",
        "\n",
        "# Step 2: Define the Multi-Head Attention Layer\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        assert d_model % num_heads == 0\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.wq(query).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = self.wk(key).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = self.wv(value).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / torch.sqrt(self.head_dim)\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.nn.functional.softmax(energy, dim=-1)\n",
        "        x = torch.matmul(attention, V)\n",
        "        x = x.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Step 3: Define the Position-wise Feedforward Layer\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Step 4: Define the Encoder Layer\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.multihead_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.positionwise_ff = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attention_output = self.multihead_attention(x, x, x, mask)\n",
        "        x = x + self.dropout(attention_output)\n",
        "        x = self.layer_norm1(x)\n",
        "        feed_forward_output = self.positionwise_ff(x)\n",
        "        x = x + self.dropout(feed_forward_output)\n",
        "        x = self.layer_norm2(x)\n",
        "        return x\n",
        "\n",
        "# Step 5: Define the Transformer Encoder\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, d_ff, max_seq_len, dropout=0.1):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([TransformerEncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = x + self.positional_encoding\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n",
        "\n",
        "# Step 6: Create the Big Transformer Model\n",
        "class BigTransformerModel(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, d_ff, max_seq_len, num_classes, dropout=0.1):\n",
        "        super(BigTransformerModel, self).__init__()\n",
        "        self.encoder = TransformerEncoder(num_layers, d_model, num_heads, d_ff, max_seq_len, dropout)\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.encoder(x, mask)\n",
        "        x = x.mean(dim=1)  # Global average pooling\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Step 7: Instantiate the Model\n",
        "num_layers = 6\n",
        "d_model = 1024\n",
        "num_heads = 16\n",
        "d_ff = 4096\n",
        "max_seq_len = 512\n",
        "num_classes = 10  # Change this to your desired number of output classes\n",
        "dropout = 0.1\n",
        "\n",
        "model = BigTransformerModel(num_layers, d_model, num_heads, d_ff, max_seq_len, num_classes, dropout)\n",
        "\n",
        "# Now you can use this model for your specific task, such as text classification or machine translation, by feeding data through it."
      ],
      "metadata": {
        "id": "efFKDDaW2KO2"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ilLOqyyExdUa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 步骤 1: 定义位置编码（Positional Encoding）\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len=512):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # 生成位置编码的位置索引（0到max_seq_len-1）\n",
        "        position = torch.arange(0, max_seq_len).unsqueeze(1)\n",
        "\n",
        "        # 计算位置编码的分母项\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "\n",
        "        # 创建位置编码矩阵，初始化为全零\n",
        "        self.positional_encoding = torch.zeros(1, max_seq_len, d_model)\n",
        "\n",
        "        # 使用正弦和余弦函数来填充位置编码矩阵\n",
        "        self.positional_encoding[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        self.positional_encoding[0, :, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 在输入张量x上加上位置编码\n",
        "        return x + self.positional_encoding\n",
        "\n",
        "\n",
        "# 步骤 2: 定义多头自注意力层（Multi-Head Attention Layer）\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        # 设置多头注意力的头数和每个头的维度\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        # 确保模型维度能够被头数整除\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        # 创建权重矩阵，用于查询（Q）、键（K）、值（V）的线性变换\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # 用于最终输出的线性变换\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        # 对查询、键、值进行线性变换并重塑张量以获得多头维度\n",
        "        Q = self.wq(query).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = self.wk(key).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = self.wv(value).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # 计算注意力得分\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / torch.sqrt(self.head_dim)\n",
        "\n",
        "        # 如果存在遮罩（mask），则将注意力得分中的特定位置替换为负无穷\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        # 使用 softmax 函数获得归一化的注意力权重\n",
        "        attention = torch.nn.functional.softmax(energy, dim=-1)\n",
        "\n",
        "        # 根据注意力权重计算加权和\n",
        "        x = torch.matmul(attention, V)\n",
        "\n",
        "        # 重塑张量以恢复原始形状\n",
        "        x = x.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)\n",
        "\n",
        "        # 使用全连接层进行最终线性变换\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# 步骤 3: 定义位置前馈层（Position-wise Feedforward Layer）\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        # 使用全连接层 fc1 将输入维度 d_model 转换为 d_ff 维度\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        # 使用全连接层 fc2 将 d_ff 维度转换回 d_model 维度\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 应用 ReLU 激活函数\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        # 通过 fc2 进行另一次线性变换\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 步骤 4: 定义编码器层（Encoder Layer）\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        # 多头自注意力层，传入输入维度 d_model 和注意头数 num_heads\n",
        "        self.multihead_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        # 位置前馈层，传入输入维度 d_model 和 d_ff 维度\n",
        "        self.positionwise_ff = PositionWiseFeedForward(d_model, d_ff)\n",
        "        # 第一个层归一化（LN）层\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        # 第二个层归一化（LN）层\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "        # 用于增强模型的 dropout 操作\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # 使用多头自注意力层处理输入 x，mask 用于遮蔽无关信息\n",
        "        attention_output = self.multihead_attention(x, x, x, mask)\n",
        "        # 将自注意力层的输出与输入相加，用于残差连接\n",
        "        x = x + self.dropout(attention_output)\n",
        "        # 应用第一个 LN 层，用于规范化\n",
        "        x = self.layer_norm1(x)\n",
        "        # 使用位置前馈层处理输出 x\n",
        "        feed_forward_output = self.positionwise_ff(x)\n",
        "        # 将前馈层的输出与输入相加，用于残差连接\n",
        "        x = x + self.dropout(feed_forward_output)\n",
        "        # 应用第二个 LN 层，用于规范化\n",
        "        x = self.layer_norm2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# 步骤 5: 定义Transformer编码器（Transformer Encoder）\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, d_ff, max_seq_len, dropout=0.1):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        # 创建一个由多个TransformerEncoderLayer组成的列表，并将它们堆叠起来\n",
        "        self.layers = nn.ModuleList([TransformerEncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        # 添加位置编码器，用于处理序列的位置信息\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # 将位置编码添加到输入张量中\n",
        "        x = x + self.positional_encoding\n",
        "        # 通过多个TransformerEncoderLayer来编码输入\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n",
        "\n",
        "# 步骤 6: 创建大型Transformer模型\n",
        "class BigTransformerModel(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, d_ff, max_seq_len, num_classes, dropout=0.1):\n",
        "        super(BigTransformerModel, self).__init__()\n",
        "        # 创建一个Transformer编码器，用于处理输入序列\n",
        "        self.encoder = TransformerEncoder(num_layers, d_model, num_heads, d_ff, max_seq_len, dropout)\n",
        "        # 添加一个全连接层，用于将Transformer编码器的输出映射到类别标签\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # 将位置编码添加到输入张量中\n",
        "        x = x + self.encoder.positional_encoding\n",
        "        # 通过Transformer编码器处理输入\n",
        "        x = self.encoder(x, mask)\n",
        "        # 进行全局平均池化，以获得最终的输出表示\n",
        "        x = x.mean(dim=1)\n",
        "        # 使用全连接层进行最终的分类\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# 步骤 7: 实例化模型\n",
        "num_layers = 6  # Transformer 编码器中的层数\n",
        "d_model = 1024  # 模型的维度\n",
        "num_heads = 16  # 多头自注意力机制中的头数\n",
        "d_ff = 4096  # 位置前馈层的隐藏层维度\n",
        "max_seq_len = 512  # 最大序列长度\n",
        "num_classes = 10  # 将此值更改为您所需的输出类别数\n",
        "dropout = 0.1  # 随机失活概率\n",
        "\n",
        "model = BigTransformerModel(num_layers, d_model, num_heads, d_ff, max_seq_len, num_classes, dropout)\n",
        "# 现在，您可以通过模型将数据馈送到特定任务，如文本分类或机器翻译中。"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8s6DXtGJ0nmi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}