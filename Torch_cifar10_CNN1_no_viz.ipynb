{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvRwdj0CglXnvetid5E7sN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/byrcewang/DL_SS2H/blob/main/Torch_cifar10_CNN1_no_viz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Define the transformation to apply to the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Sample 1% of the data\n",
        "sample_size = int(0.01 * len(trainset))\n",
        "subset_indices = np.random.choice(len(trainset), sample_size, replace=False)\n",
        "trainset = torch.utils.data.Subset(trainset, subset_indices)\n",
        "\n",
        "# Define data loaders\n",
        "batch_size = 64\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define the CNN model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / (i + 1)}\")\n",
        "\n",
        "print(\"Finished Training\")\n",
        "\n",
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Accuracy on test set: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tak1jAm4lkqR",
        "outputId": "aaf81531-8400-4bef-95ea-5f6cb73c5e69"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 2.3045651614665985\n",
            "Epoch 2, Loss: 2.304522067308426\n",
            "Epoch 3, Loss: 2.3043034374713898\n",
            "Epoch 4, Loss: 2.3038435876369476\n",
            "Epoch 5, Loss: 2.3037320971488953\n",
            "Epoch 6, Loss: 2.3030445873737335\n",
            "Epoch 7, Loss: 2.3032852113246918\n",
            "Epoch 8, Loss: 2.302635431289673\n",
            "Epoch 9, Loss: 2.3023113310337067\n",
            "Epoch 10, Loss: 2.3024041950702667\n",
            "Finished Training\n",
            "Accuracy on test set: 10.49%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## With Chinese comments.\n",
        "\n",
        "# 导入必要的库\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 设置一个随机种子以实现可重现性\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# 定义要应用到数据的转换\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # 将图像转换为张量\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 对图像进行标准化\n",
        "])\n",
        "\n",
        "# 加载CIFAR-10数据集\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)  # 训练集\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)  # 测试集\n",
        "\n",
        "# 从训练集中随机采样10%的数据\n",
        "sample_size = int(0.1 * len(trainset))\n",
        "subset_indices = np.random.choice(len(trainset), sample_size, replace=False)\n",
        "trainset = torch.utils.data.Subset(trainset, subset_indices)\n",
        "\n",
        "# 定义数据加载器\n",
        "batch_size = 64\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)  # 训练数据加载器\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)  # 测试数据加载器\n",
        "\n",
        "# 定义卷积神经网络模型\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)  # 第一个卷积层\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # 最大池化层\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)  # 第二个卷积层\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 第一个全连接层\n",
        "        self.fc2 = nn.Linear(120, 84)  # 第二个全连接层\n",
        "        self.fc3 = nn.Linear(84, 10)  # 输出层\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # 第一个卷积层后面跟着ReLU激活函数和池化层\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # 第二个卷积层后面跟着ReLU激活函数和池化层\n",
        "        x = x.view(-1, 16 * 5 * 5)  # 将张量展平\n",
        "        x = F.relu(self.fc1(x))  # 第一个全连接层后面跟着ReLU激活函数\n",
        "        x = F.relu(self.fc2(x))  # 第二个全连接层后面跟着ReLU激活函数\n",
        "        x = self.fc3(x)  # 输出层\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterion = nn.CrossEntropyLoss()  # 交叉熵损失\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  # 随机梯度下降优化器\n",
        "\n",
        "# 训练模型\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / (i + 1)}\")\n",
        "\n",
        "print(\"训练完成\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNLuZ_AXmTGT",
        "outputId": "637eb12d-34ba-4d77-9af7-cf30532dbeac"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 2.3052160015589074\n",
            "Epoch 2, Loss: 2.3037749181819867\n",
            "Epoch 3, Loss: 2.301999587046949\n",
            "Epoch 4, Loss: 2.3008678080160405\n",
            "Epoch 5, Loss: 2.299054595488536\n",
            "Epoch 6, Loss: 2.2970851071273226\n",
            "Epoch 7, Loss: 2.2943187635156175\n",
            "Epoch 8, Loss: 2.2905233479753324\n",
            "Epoch 9, Loss: 2.2849812507629395\n",
            "Epoch 10, Loss: 2.2752097105678124\n",
            "训练完成\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 测试模型\n",
        "# 初始化正确分类的计数器和总样本数计数器\n",
        "correct = 0  # 用于记录正确分类的样本数量\n",
        "total = 0  # 用于记录总共处理的样本数量\n",
        "\n",
        "# 使用torch.no_grad()上下文管理器，禁用梯度计算，因为在测试中我们不需要计算梯度\n",
        "with torch.no_grad():\n",
        "    # 遍历测试数据加载器(testloader)，它会产生测试集中的小批量数据\n",
        "    for data in testloader:\n",
        "        images, labels = data  # 从数据加载器中获取测试图像和对应的标签\n",
        "\n",
        "        # 使用训练好的神经网络(net)进行前向传播，得到预测结果(outputs)\n",
        "        outputs = net(images)\n",
        "\n",
        "        # 使用torch.max函数，沿着维度1（通常是类别维度），找到每个样本中具有最高预测概率的类别\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # 更新总样本数计数器\n",
        "        total += labels.size(0)  # labels.size(0)返回当前批量中的样本数量\n",
        "\n",
        "        # 使用(predicted == labels)比较预测值与实际标签，返回一个布尔张量\n",
        "        # 使用sum().item()将True的数量相加，以得到正确分类的样本数量\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# 计算模型在测试集上的准确率\n",
        "accuracy = 100 * correct / total  # 使用正确分类的样本数和总样本数计算百分比准确率\n",
        "\n",
        "# 打印测试集准确率\n",
        "print(f\"测试集准确率: {accuracy}%\")  # 将准确率以百分比形式打印出来"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WpaqbuvnwuL",
        "outputId": "988f8cb1-2778-4d2f-93fe-0baefd064a8a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "测试集准确率: 16.84%\n"
          ]
        }
      ]
    }
  ]
}